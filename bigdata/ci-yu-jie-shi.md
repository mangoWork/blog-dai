### 机器学习中的词语解释
#### 什么是熵？
&nbsp;　　在信息论中，熵是对不确定性的测量，熵越高，则能传输越多的信息；熵越低，则意味着传输的信息越少。熵度衡量系统的不确定性，当对某个系统的知识缺乏的时候，其中的不确定性也会随着增加。
&nbsp;　　例如抛硬币，在理想的情况下是无法预测出现正面还是反面，此时熵达到最大。但是对于“明天太阳从东方升起”，完全可以依靠目前的只是，预测该事件肯定会发生，信息熵最小。对应的计算公式（只包含一个随即变量）如下所示：
&nbsp;　　香农给出了熵数学表达：某个事件用随机变量X表示，其可以的取值{x1, x2, ...xn }，则该事件的信息熵定义为:
    $$    
    H(X) = E(I(X))
    $$
    
&nbsp;　　其中I(X)，表示随机变量的信息，I(X)一般定义为：
    $$
    I(X = x_i) = -\log_2{p(x_i)}
    $$
&nbsp;　　那么，熵的定义为：
    $$
    H(X) = \sum_{i=1}^n{p(x_i)I(x_i)} = - \sum_{i=1}^n{p(x_i)\log_b{p(x_i)}}
    $$
&nbsp;　　下图给出了二分类问题熵函数：

![](./img/Binary_entropy_plot.png)

#### 条件熵
&nbsp;　　从某小学的学生中任选一人称他（她）的体重，其体重就是个随机变量，它就有个概率分布函数存在（不同的体重的出现概率不同）。如果仅对身高为1.2-1.3米的学生抽样称其体重，就得到另外一个概率分布函数。相对前一种概率分布，后者就是条件概率分布。条件就是已经知道了学生身高是1.2-1.3米。根据条件概率，利用熵公式计算的信息熵称为条件熵。
&nbsp;　　如果以x表示学生体重，以y表示身高，以 p(x∣y表示身高为y时的体重为x 的出现的概率，把熵公式用到这个特殊情况得到是熵显然应当是:
$$
H(x|y_j) = -\sum_{i=1}^n{p(x_i|y_j)logp(x_i|y_i)}
$$
&nbsp;　　上面得到的计算公式是针对y为一个特殊值y时求得的熵。考虑到y会出现各种可能值，如果问已知学生身高时（不特指某一身高，而是泛指身高已经知道）的体重的熵（不确定程度），它应当是把前面的公式依各种y的出现概率做加权平均。即:
$$
H(x|y) = -\sum_{i=1}^n\sum_{j=1}^m{p(x_i, y_j)log{\frac{p(x_i, y_i)}{p(y_j)}}} 
$$
$$
H(x|y) = -\iint{f(x, y)log{\frac{x, y}{f(y)}}}
$$

#### 信息增益
&nbsp;　　信息增益是一种衡量样本特征重要性的方法，直观的理解是有无样本对分类问题的影响大小。假设某个状态下系统的信息熵为H(Y)，再引入某个特征X后的信息熵为H(Y|X)，则特征X的信息增益定义为：
$$
IG(Y|X) = H(Y) - H(X)
$$

||特征A|特征B|特征C|类别|
|:-----:|:-----:|:-----:|:-----:|
|samp1	|0	|1|	0|	0
|samp2	|0	|0	|1|	0
|samp3	|1	|0	|0|	1
|samp4	|1	|0	|1|	1